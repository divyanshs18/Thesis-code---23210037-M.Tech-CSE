# -*- coding: utf-8 -*-
"""MP2GAZE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJ-MSqxS8Vy_2sSGxSKK8W9or86tUCJZ
"""

#3D GAZE ESTIMATION FULL CODE

pip install --upgrade pip

import cv2
import torch
import torch.nn as nn
import numpy as np
import logging
import time
import random


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class DummyGazeCNN(nn.Module):
    def __init__(self):
        super(DummyGazeCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 8, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(8, 16, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(16 * 73 * 73, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # Output gaze (x, y)
        )

    def forward(self, x):
        return self.conv(x)


def preprocess_frame(frame):
    frame_resized = cv2.resize(frame, (300, 300))
    frame_tensor = torch.from_numpy(frame_resized).float() / 255.0
    frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
    return frame_tensor

def draw_gaze_point(frame, gaze_point):
    x, y = int(gaze_point[0]), int(gaze_point[1])
    if 0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]:
        cv2.circle(frame, (x, y), 8, (0, 255, 0), -1)
    return frame


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    model = DummyGazeCNN().to(device)
    model.eval()

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        logging.error("Cannot open webcam.")
        return

    logging.info("Starting webcam feed. Press 'q' to quit.")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                logging.warning("Failed to read frame from webcam.")
                break

            input_tensor = preprocess_frame(frame).to(device)

            # Fake gaze prediction (simulate)
            with torch.no_grad():
                # Simulated output  random point on screen
                h, w = frame.shape[:2]
                gaze_x = random.randint(0, w)
                gaze_y = random.randint(0, h)
                gaze_point = (gaze_x, gaze_y)

            # Draw gaze point
            frame_with_gaze = draw_gaze_point(frame, gaze_point)

            cv2.imshow("Gaze Estimation (Simulated)", frame_with_gaze)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()
        logging.info("Webcam closed. Exiting program.")

if __name__ == "__main__":
    main()

import cv2
import torch
import torch.nn as nn
import numpy as np
import logging
import time
import random


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class DummyGazeCNN(nn.Module):
    def __init__(self):
        super(DummyGazeCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 8, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(8, 16, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(16 * 73 * 73, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # Output gaze (x, y)
        )

    def forward(self, x):
        return self.conv(x)


def preprocess_frame(frame):
    frame_resized = cv2.resize(frame, (300, 300))
    frame_tensor = torch.from_numpy(frame_resized).float() / 255.0
    frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
    return frame_tensor

def draw_gaze_point(frame, gaze_point):
    x, y = int(gaze_point[0]), int(gaze_point[1])
    if 0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]:
        cv2.circle(frame, (x, y), 8, (0, 255, 0), -1)
    return frame


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    model = DummyGazeCNN().to(device)
    model.eval()

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        logging.error("Cannot open webcam.")
        return

    logging.info("Webcam started. Press 'q' or ESC to quit.")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                logging.warning("Failed to read frame from webcam.")
                break

            input_tensor = preprocess_frame(frame).to(device)

            # Simulated gaze point
            h, w = frame.shape[:2]
            gaze_point = (random.randint(0, w - 1), random.randint(0, h - 1))

            frame_with_gaze = draw_gaze_point(frame, gaze_point)

            cv2.imshow("Gaze Estimation (Simulated)", frame_with_gaze)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27:  # 'q' or ESC
                logging.info("Exit key pressed.")
                break

    except KeyboardInterrupt:
        logging.info("KeyboardInterrupt received. Exiting.")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        logging.info("Resources released. Program ended.")

if __name__ == "__main__":
    main()

import cv2
import torch
import torch.nn as nn
import numpy as np
import logging
import time
import random


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class DummyGazeCNN(nn.Module):
    def __init__(self):
        super(DummyGazeCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 8, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(8, 16, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(16 * 73 * 73, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # Output gaze (x, y)
        )

    def forward(self, x):
        return self.conv(x)


def preprocess_frame(frame):
    frame_resized = cv2.resize(frame, (300, 300))
    frame_tensor = torch.from_numpy(frame_resized).float() / 255.0
    frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)
    return frame_tensor


def draw_gaze_point(frame, gaze_point, frame_count, timestamp):
    x, y = int(gaze_point[0]), int(gaze_point[1])
    if 0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]:
        cv2.circle(frame, (x, y), 8, (0, 255, 0), -1)

    # Overlay text info
    cv2.putText(frame, f"Frame: {frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
    cv2.putText(frame, f"Time: {timestamp:.1f}s", (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
    cv2.putText(frame, f"Gaze: ({x}, {y})", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

    return frame


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    model = DummyGazeCNN().to(device)
    model.eval()

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        logging.error("Cannot open webcam.")
        return

    logging.info("Webcam started. Auto-quit after 60 seconds or press 'q'/'ESC' to quit early.")

    start_time = time.time()
    frame_count = 0

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                logging.warning("Failed to read frame.")
                break

            elapsed = time.time() - start_time
            if elapsed > 60:
                logging.info("Time limit reached (60 seconds). Exiting.")
                break

            input_tensor = preprocess_frame(frame).to(device)


            h, w = frame.shape[:2]
            gaze_point = (random.randint(0, w - 1), random.randint(0, h - 1))

            frame_count += 1
            frame_with_overlay = draw_gaze_point(frame, gaze_point, frame_count, elapsed)

            cv2.imshow("Eye Tracking (Simulated Gaze)", frame_with_overlay)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27:  # 'q' or ESC
                logging.info("Exit key pressed by user.")
                break

    except KeyboardInterrupt:
        logging.info("Keyboard interrupt. Exit.")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        logging.info("Camera released and window closed.")

if __name__ == "__main__":
    main()

import cv2
import time
import random

def main():
    cam_idx = 0
    cap = cv2.VideoCapture(cam_idx)
    if not cap.isOpened():
        print("Error: Could not open webcam.")
        return

    frame_count = 0
    start_time = time.time()
    max_duration = 60  # seconds

    # To store detailed gaze data for each frame
    gaze_log = []

    print("[INFO] Starting gaze simulation. The window will close automatically after 60 seconds.")
    print("[INFO] Press 'q' or ESC to quit before.\n")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("Error: Could not read frame.")
            break

        frame_count += 1
        elapsed_time = time.time() - start_time

        # Simulated gaze point
        h, w = frame.shape[:2]
        gaze_x = random.randint(0, w - 1)
        gaze_y = random.randint(0, h - 1)

        # gaze point on the frame
        cv2.circle(frame, (gaze_x, gaze_y), 8, (0, 255, 0), -1)

        # Overlay text: Frame count, elapsed time, gaze coordinates
        cv2.putText(frame, f"Frame: {frame_count}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        cv2.putText(frame, f"Time: {elapsed_time:.2f}s", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        cv2.putText(frame, f"Gaze: ({gaze_x}, {gaze_y})", (10, 90),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

        # Saving current data to log
        gaze_log.append({
            'frame': frame_count,
            'time_sec': elapsed_time,
            'gaze_x': gaze_x,
            'gaze_y': gaze_y
        })

        # Show frame
        cv2.imshow("Eye Tracking (Simulated Gaze)", frame)

        # Exit if 'q' or ESC pressed or time exceeded 60 seconds
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q') or key == 27 or elapsed_time > max_duration:
            break

    cap.release()
    cv2.destroyAllWindows()

    # Print detailed gaze log summary
    print("\n[INFO] Gaze Tracking final data:")
    for entry in gaze_log:
        print(f"Frame {entry['frame']:4d} | Time: {entry['time_sec']:6.2f}s | Gaze: ({entry['gaze_x']:4d}, {entry['gaze_y']:4d})")

    print("\n[INFO] Program finished.")

if __name__ == "__main__":
    main()

#####

#MP2FACEGAZE

#running on one folder for testing

import cv2
import time
import os
from PIL import Image
import numpy as np
from torchvision import transforms


DATASET_ROOT = '/Users/divyanshsaini/Documents/eyeteackipynb/MPIIFaceGaze/p00/day06'

# Load all PNG images from dataset folder
image_files = [f for f in os.listdir(DATASET_ROOT) if f.lower().endswith('.png')]
image_files.sort()

)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])


class DummyGazeModel:
    def predict(self, img_tensor):
        return np.random.rand(2)

model = DummyGazeModel()

def predict_gaze(image_pil):
    img_tensor = transform(image_pil).unsqueeze(0)
    gaze_pred = model.predict(img_tensor)
    return gaze_pred


print("Running gaze estimation on dataset images (no labels)...")
for img_file in image_files:
    img_path = os.path.join(DATASET_ROOT, img_file)
    img = Image.open(img_path).convert('RGB')
    gaze_pred = predict_gaze(img)
    print(f"Image: {img_file} | Predicted gaze (normalized): ({gaze_pred[0]:.3f}, {gaze_pred[1]:.3f})")

print("Dataset gaze prediction complete.\n")

# -----------------
# 2) Live webcam gaze tracking with auto quit after 1 minute

cap = cv2.VideoCapture(0)
start_time = time.time()
duration = 60  # seconds

print("Starting live webcam gaze tracking. Press 'q' to quit earlier.")

while True:
    ret, frame = cap.read()
    if not ret:
        print("Failed to capture frame from webcam.")
        break

    # Convert to PIL Image for prediction
    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    gaze_pred = predict_gaze(frame_pil)

    # Map normalized gaze to frame coordinates
    screen_x = int(gaze_pred[0] * frame.shape[1])
    screen_y = int(gaze_pred[1] * frame.shape[0])

    # Draw circle on gaze point
    cv2.circle(frame, (screen_x, screen_y), 15, (0, 255, 0), -1)
    cv2.putText(frame, f"Gaze: ({gaze_pred[0]:.2f}, {gaze_pred[1]:.2f})",
                (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)

    cv2.imshow('Live Gaze Estimation', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        print("User requested exit.")
        break

    if time.time() - start_time > duration:
        print("Auto quitting after 1 minute.")
        break

cap.release()
cv2.destroyAllWindows()
print("Live gaze tracking ended.")

#running on full dataset

import os
from PIL import Image
import numpy as np
from torchvision import transforms

#root directory
DATASET_ROOT = '/Users/divyanshsaini/Documents/eyeteackipynb/MPIIFaceGaze'

# Recursively collect all PNG images inside dataset root and subfolders
def get_all_images(root_dir, exts=['.jpg', '.JPG']):
    image_paths = []
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if any(file.endswith(ext) for ext in exts):
                full_path = os.path.join(subdir, file)
                image_paths.append(full_path)
    return sorted(image_paths)

class DummyGazeModel:
    def predict(self, img_tensor):
        # Random gaze coords normalized [0,1]
        return np.random.rand(2)


transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])

def predict_gaze(image_pil):
    img_tensor = transform(image_pil).unsqueeze(0)  # batch dimension
    gaze_pred = model.predict(img_tensor)
    return gaze_pred


model = DummyGazeModel()


def process_dataset(dataset_path):
    all_images = get_all_images(dataset_path)
    print(f"Found {len(all_images)} JPG images in dataset (including subfolders).\n")

    for idx, img_path in enumerate(all_images, 1):
        try:
            img = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"[{idx}] Failed to load image {img_path}: {e}")
            continue

        gaze = predict_gaze(img)
        width, height = img.size

        print(f"[{idx}] Image: {img_path}")
        print(f"    - Resolution: {width} x {height}")
        print(f"    - Predicted gaze (normalized): X={gaze[0]:.4f}, Y={gaze[1]:.4f}")
        print(f"    - Predicted gaze (pixel coords): X={int(gaze[0]*width)}, Y={int(gaze[1]*height)}\n")


process_dataset(DATASET_ROOT)

##Pre- trained cnn model for mp2 face gaze

##code


import os
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision import models
import numpy as np
import pandas as pd
from tqdm import tqdm

# ---------- CONFIG ----------
DATASET_PATH = "/Users/divyanshsaini/Documents/eyeteackipynb/MPIIFaceGaze"
OUTPUT_CSV = "predicted_gaze_results.csv"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
IMAGE_SIZE = (224, 224)  # ResNet input
# ----------------------------

# Pretrained CNN (ResNet18 )
class GazeEstimator(nn.Module):
    def __init__(self):
        super(GazeEstimator, self).__init__()
        base_model = models.resnet18(pretrained=True)
        num_features = base_model.fc.in_features
        base_model.fc = nn.Identity()
        self.features = base_model
        self.fc = nn.Sequential(
            nn.Linear(num_features, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # Output: (x_gaze, y_gaze)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.fc(x)
        return x

# Image pre processing
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])  # Imagenet mean/std
])

# Load model
model = GazeEstimator().to(DEVICE)
model.eval()


def get_all_images(root_folder):
    image_paths = []
    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths

# Predict gaze
def process_dataset(dataset_path):
    image_paths = get_all_images(dataset_path)
    results = []

    for img_path in tqdm(image_paths, desc="Processing Images"):
        img = cv2.imread(img_path)
        if img is None:
            continue
        h, w, _ = img.shape
        img_tensor = transform(img).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            gaze_coords = model(img_tensor).cpu().numpy()[0]
            pixel_coords = [gaze_coords[0] * w, gaze_coords[1] * h]

        results.append({
            "image_path": img_path,
            "resolution": f"{w}x{h}",
            "gaze_x": gaze_coords[0],
            "gaze_y": gaze_coords[1],
            "gaze_pixel_x": pixel_coords[0],
            "gaze_pixel_y": pixel_coords[1]
        })

    return results

# Run processing and save to CSV
results = process_dataset(DATASET_PATH)
df = pd.DataFrame(results)
df.to_csv(OUTPUT_CSV, index=False)
print(f"\n  Predictions saved to {OUTPUT_CSV}")

OUTPUT_CSV = "/Users/divyanshsaini/Documents/eyeteackipynb/MPIIFaceGaze/output"

import os
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision import models
import numpy as np
import pandas as pd
from tqdm import tqdm

# ---------- CONFIG ----------
DATASET_PATH = "/Users/divyanshsaini/Documents/eyeteackipynb/MPIIFaceGaze"
OUTPUT_CSV = "/Users/divyanshsaini/Documents/eyeteackipynb/MPIIFaceGaze/output/predicted_gaze_results.csv"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
IMAGE_SIZE = (224, 224)
# ----------------------------

# Pretrained ResNet-based Gaze Estimator
class GazeEstimator(nn.Module):
    def __init__(self):
        super(GazeEstimator, self).__init__()
        base_model = models.resnet18(pretrained=True)
        num_features = base_model.fc.in_features
        base_model.fc = nn.Identity()
        self.features = base_model
        self.fc = nn.Sequential(
            nn.Linear(num_features, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # Output: gaze_x,y (normalized)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.fc(x)
        return x

# Image preprocessing
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])  # Imagenet mean
])

# Load model
model = GazeEstimator().to(DEVICE)
model.eval()

# Collect all image paths
def get_all_images(root_folder):
    image_paths = []
    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith('.jpg'):
                image_paths.append(os.path.join(root, file))
    return image_paths

# Predict gaze
def process_dataset(dataset_path):
    image_paths = get_all_images(dataset_path)
    results = []

    for img_path in tqdm(image_paths, desc="Processing Images"):
        img = cv2.imread(img_path)
        if img is None:
            continue
        h, w, _ = img.shape
        img_tensor = transform(img).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            gaze_coords = model(img_tensor).cpu().numpy()[0]
            gaze_x, gaze_y = gaze_coords
            gaze_pixel_x = gaze_x * w
            gaze_pixel_y = gaze_y * h

        results.append({
            "image_path": img_path,
            "resolution": f"{w}x{h}",
            "gaze_x_normalized": gaze_x,
            "gaze_y_normalized": gaze_y,
            "gaze_pixel_x": gaze_pixel_x,
            "gaze_pixel_y": gaze_pixel_y
        })

    return results


results = process_dataset(DATASET_PATH)
df = pd.DataFrame(results)


os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)

df.to_csv(OUTPUT_CSV, index=False)
print(f"\n  Predictions saved to: {os.path.abspath(OUTPUT_CSV)}")

