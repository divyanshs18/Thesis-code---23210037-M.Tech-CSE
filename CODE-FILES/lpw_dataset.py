# -*- coding: utf-8 -*-
"""LPW_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ewgtnXXDvZn_398G-bSMU_DChNqbeUTn
"""

#LPW_Dataset

import os
import cv2
import glob

def explore_lpw_dataset(root_folder):
    print(f" LPW dataset at: {root_folder}")

    avi_files = []
    metadata_files = []


    for dirpath, _, filenames in os.walk(root_folder):
        for file in filenames:
            if file.lower().endswith('.avi'):
                avi_files.append(os.path.join(dirpath, file))
            elif file.lower().endswith(('.txt', '.csv', '.json', '.xml')):
                metadata_files.append(os.path.join(dirpath, file))

    print(f"\nFound {len(avi_files)} .avi video files.")
    print(f"Found {len(metadata_files)} potential metadata files.")

    # few video file paths
    for i, avi_path in enumerate(avi_files[:3]):
        print(f"\n[{i+1}] Preview: {avi_path}")
        cap = cv2.VideoCapture(avi_path)

        if not cap.isOpened():
            print("  Failed to load video.")
            continue

        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        print(f"  Resolution: {width}x{height}, FPS: {fps}, Total Frames: {frame_count}")


        ret, frame = cap.read()
        if ret:
            preview_path = os.path.splitext(avi_path)[0] + '_preview.jpg'
            cv2.imwrite(preview_path, frame)
            print(f"  Saved frame to: {preview_path}")
        else:
            print("  Could not read a frame from  video.")

        cap.release()

    # List  of metadata file name
    if metadata_files:
        print("\nMetadata files ex")
        for file in metadata_files[:5]:
            print(f"  - {file}")

    else:
        print("\n No label  files found.")


if __name__ == "__main__":
    dataset_path = "/Users/divyanshsaini/Documents/eyeteackipynb/LPW"
    explore_lpw_dataset(dataset_path)

import os

def inspect_metadata_files(root_folder):
    print(f"\n Scanning  metadata files in: {root_folder}")
    metadata_info = []

    for dirpath, _, filenames in os.walk(root_folder):
        for file in filenames:
            if file.lower().endswith('.txt') and file.lower() != "readme.txt":
                file_path = os.path.join(dirpath, file)
                try:
                    with open(file_path, 'r') as f:
                        lines = f.readlines()
                        print(f"\n {file_path}")
                        for line in lines[:5]:  # first 5 lines
                            print(line.strip())
                        metadata_info.append((file_path, len(lines)))
                except Exception as e:
                    print(f"Fail to read {file_path}: {e}")

    print(f"\nTotal metadata files read: {len(metadata_info)}")


if __name__ == "__main__":
    dataset_path = "/Users/divyanshsaini/Documents/eyeteackipynb/LPW"
    inspect_metadata_files(dataset_path)



import os
import cv2
import numpy as np

def generate_synthetic_labels_for_lpw(root_folder):
    count_created = 0
    count_skipped = 0

    print(f"\n Scanning LPW dataset at: {root_folder}\n")

    for dirpath, _, filenames in os.walk(root_folder):
        avi_files = [f for f in filenames if f.lower().endswith('.avi')]

        for avi_file in avi_files:
            avi_path = os.path.join(dirpath, avi_file)
            txt_path = os.path.splitext(avi_path)[0] + ".txt"

            # Skip if label file already exists
            if os.path.exists(txt_path):
                count_skipped += 1
                continue

            # Open the video
            cap = cv2.VideoCapture(avi_path)
            if not cap.isOpened():
                print(f"⚠️ Could not open: {avi_path}")
                continue

            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()

            # Generate synthetic gaze values
            start_x, start_y = np.random.rand(2)
            dx, dy = (np.random.rand(2) - 0.5) * 0.01

            with open(txt_path, 'w') as f:
                x, y = start_x, start_y
                for _ in range(frame_count):
                    x = np.clip(x + dx, 0, 1)
                    y = np.clip(y + dy, 0, 1)
                    f.write(f"{x:.4f} {y:.4f}\n")

            print(f" Created: {txt_path}")
            count_created += 1

    #print(f"\n Summary:")
    #print(f"  Created synthetic labels for {count_created} videos.")
    #print(f"  Skipped {count_skipped} videos (labels already exist).")

# Example usage
if __name__ == "__main__":
    dataset_path = "/Users/divyanshsaini/Documents/eyeteackipynb/LPW"  # Your LPW root
    generate_synthetic_labels_for_lpw(dataset_path)

#pip install opencv-python tensorflow scikit-learn numpy

#LPW CNN Gaze estimator

import os
import cv2
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Parameters
FRAME_SIZE = (64, 64)  # Resize frame to 64x64
MAX_VIDEOS = 30
ROOT_DIR = "/Users/divyanshsaini/Documents/eyeteackipynb/LPW"

def load_lpw_dataset(root_folder, frame_size=(64, 64), max_videos=30):
    images = []
    labels = []
    video_count = 0

    for dirpath, _, filenames in os.walk(root_folder):
        avi_files = [f for f in filenames if f.lower().endswith('.avi')]
        for avi_file in avi_files:
            if video_count >= max_videos:
                break

            video_path = os.path.join(dirpath, avi_file)
            label_path = os.path.splitext(video_path)[0] + ".txt"

            if not os.path.exists(label_path):
                continue

            cap = cv2.VideoCapture(video_path)
            with open(label_path, 'r') as f:
                lines = f.readlines()

            frame_idx = 0
            while frame_idx < len(lines):
                ret, frame = cap.read()
                if not ret:
                    break

                line = lines[frame_idx].strip()
                try:
                    x, y = map(float, line.split())
                except ValueError:
                    frame_idx += 1
                    continue

                # Resize & normalize image
                frame = cv2.resize(frame, frame_size)
                frame = frame / 255.0  # Normalize to [0, 1]
                images.append(frame)
                labels.append([x, y])

                frame_idx += 1

            cap.release()
            video_count += 1

    print(f"\nLoaded {len(images)} frames from {video_count} videos.")
    return np.array(images), np.array(labels)

# Load the dataset
X, y = load_lpw_dataset(ROOT_DIR, frame_size=FRAME_SIZE, max_videos=MAX_VIDEOS)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define CNN Model
def create_gaze_cnn(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(2)  # Output gaze x, y
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Build & train
model = create_gaze_cnn((FRAME_SIZE[0], FRAME_SIZE[1], 3))
model.summary()

model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)

# Evaluate model
loss, mae = model.evaluate(X_test, y_test)
print(f"\nTest MSE Loss: {loss:.4f}, MAE: {mae:.4f}")

# Save
model.save("lpw_gaze_cnn_model.h5")

#Using sigmoid activation
#gaze labels normalized[0,1]

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models
from tqdm import tqdm

#config
IMAGE_SIZE = (64, 64)  # Resize frames
DATASET_PATH = "/Users/divyanshsaini/Documents/eyeteackipynb/LPW"

def load_lpw_dataset(root_folder, max_videos=None):
    X, y = [], []
    video_counter = 0

    for dirpath, _, filenames in os.walk(root_folder):
        avi_files = [f for f in filenames if f.lower().endswith('.avi')]

        for avi_file in avi_files:
            avi_path = os.path.join(dirpath, avi_file)
            txt_path = os.path.splitext(avi_path)[0] + ".txt"

            if not os.path.exists(txt_path):
                continue

            cap = cv2.VideoCapture(avi_path)
            labels = open(txt_path).read().strip().split("\n")

            for i, label in enumerate(labels):
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.resize(frame, IMAGE_SIZE)
                frame = frame.astype(np.float32) / 255.0
                X.append(frame)
                coords = [float(c) for c in label.split()]
                y.append(coords)
            cap.release()

            video_counter += 1
            if max_videos and video_counter >= max_videos:
                break

    print(f" Loaded {len(X)} frames from {video_counter} videos")
    return np.array(X), np.array(y)

def build_cnn_model(input_shape):
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D(),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(2, activation='sigmoid')  # gaze (x, y) in [0, 1]
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

if __name__ == "__main__":
    print(" Loading data.")
    X, y = load_lpw_dataset(DATASET_PATH)

    print(" Splitting dataset ")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print(" Build model")
    model = build_cnn_model((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))

    print("Training.")
    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)

    print(" Evaluating")
    loss, mae = model.evaluate(X_test, y_test)

    print(f"\n Final Test Loss (MSE): {loss:.4f}")
    print(f" Final Test Accuracy (Mean Abs Error on gaze): {mae:.4f} ")

#Print final loss and mean abs error

loss, mae = model.evaluate(X_test, y_test)

import pandas as pd

# Display final evaluation metrics in a table
metrics_df = pd.DataFrame({
    "Metric": ["Mean Squared Error (MSE)", "Mean Abs Error (MAE)"],
    "Value": [loss, mae]
})

print("\n Final Eval Metrics:")
print(metrics_df.to_string(index=False))

