# -*- coding: utf-8 -*-
"""Lucknow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCRMft9HCYEgjNke0tdrYQEn9bANdLER
"""

##Lucknow_Data_Shared_By_Sir

##CNN Model Implimentation

from google.colab import drive
drive.mount('/content/drive')

!pip install pyxdf

##DAY 1 Participant 1

!pip install pyxdf

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

# Config
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 1/P1 - First Exposure'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 100
BATCH_SIZE = 16
LEARNING_RATE = 0.001

def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f" Failed to load {filepath}: {e}")
    return None

def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y

# Collect all data once
def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)

# Load & split
print(" Loading data from XDF files.")
X, y = load_dataset(ROOT_DIR)
print(" Total sequences extracted:", len(X))

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Create datasets
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Build model
def build_model(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)  # x, y
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

print("Building model.")
model = build_model((SEQUENCE_LENGTH, FEATURE_DIM))

# Train
print(" Training model.")
model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

print("Training Complete")

history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print("üß™ Evaluating model on test data.")

# Convert val_ds to full NumPy arrays
X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_pred = model.predict(X_test)

# Compute standard metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f"Final Test MAE : {mae:.4f}")

# Compute Euclidean error
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
euclidean_mean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance (error in gaze point): {euclidean_mean:.4f}")


plt.figure(figsize=(10, 5))

# MSE Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train MSE')
plt.plot(history.history['val_loss'], label='Val MSE')
plt.title("MSE Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()

# MAE Plot
if 'mae' in history.history:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Train MAE')
    plt.plot(history.history['val_mae'], label='Val MAE')
    plt.title("MAE Accuracy over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Mean Absolute Error")
    plt.legend()

plt.tight_layout()
plt.show()

##accuracy of the cnn model

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# Converting train_ds and val_ds into NumPy arrays
X_train = np.concatenate([x for x, _ in train_ds], axis=0)
y_train = np.concatenate([y for _, y in train_ds], axis=0)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Prediction
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# MAE
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# Euclidean distance ( for gaze)
train_euclidean = np.mean(np.linalg.norm(y_train - y_train_pred, axis=1))
test_euclidean = np.mean(np.linalg.norm(y_test - y_test_pred, axis=1))

print(" Train Metrics:")
print(f"   MAE: {train_mae:.4f}")
print(f"   Mean Euclidean Distance: {train_euclidean:.4f}")

print("Test Metrics:")
print(f"  MAE: {test_mae:.4f}")
print(f" Mean Euclidean Distance: {test_euclidean:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print(" Evaluating model on test data")
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f" Final Test MAE: {mae:.4f}")

# Euclidean distance metric
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
mean_euclidean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance: {mean_euclidean:.4f}")

#  few sample predictions
print("\n Sample Predicted vs Actual Gaze Points:")
for i in range(5):
    print(f"Predicted: {y_pred[i]},  Actual: {y_test[i]}")

##Day 1 p02

!pip install pyxdf

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

# Config
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 1/P2 - Second Exposure'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 100
BATCH_SIZE = 16
LEARNING_RATE = 0.001

def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f" Failed to load {filepath}: {e}")
    return None

def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y

# Collect all data
def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)

# Loading & spliting
print(" Loading data from XDF files.")
X, y = load_dataset(ROOT_DIR)
print("Total sequences extracted:", len(X))

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Create datasets
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Build model
def build_model(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)  # x, y
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

print("üõ†Ô∏è Building model.")
model = build_model((SEQUENCE_LENGTH, FEATURE_DIM))

# Train
print(" Training model...")
model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

print("Training Complete")

history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

##evaluating cnn model

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print(" Evaluating model on test data...")

# Convert val_ds to full NumPy arrays
X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_pred = model.predict(X_test)

# Compute standard metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f" Final Test MAE (Accuracy Proxy): {mae:.4f}")

# Compute Euclidean error
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
euclidean_mean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance (error in gaze point): {euclidean_mean:.4f}")

# Plot training history
plt.figure(figsize=(10, 5))

# MSE Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train MSE')
plt.plot(history.history['val_loss'], label='Val MSE')
plt.title("MSE Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()

# MAE
if 'mae' in history.history:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Train MAE')
    plt.plot(history.history['val_mae'], label='Val MAE')
    plt.title("MAE Accuracy over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Mean Absolute Error")
    plt.legend()

plt.tight_layout()
plt.show()

##accuracy of cnn model for day1 P_02

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# Convert train_ds and val_ds into NumPy arrays
X_train = np.concatenate([x for x, _ in train_ds], axis=0)
y_train = np.concatenate([y for _, y in train_ds], axis=0)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# MAE
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# Euclidean distance (for better gaze prediction)
train_euclidean = np.mean(np.linalg.norm(y_train - y_train_pred, axis=1))
test_euclidean = np.mean(np.linalg.norm(y_test - y_test_pred, axis=1))

print("Train Metrics:")
print(f" MAE: {train_mae:.4f}")
print(f" Mean Euclidean Distance: {train_euclidean:.4f}")

print("Test Metrics:")
print(f"    MAE: {test_mae:.4f}")
print(f"    Mean Euclidean Distance: {test_euclidean:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print(" Evaluating model on test data.")
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f"Final Test MAE: {mae:.4f}")

# Euclidean distance metric
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
mean_euclidean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance: {mean_euclidean:.4f}")

# few sample predictions
print("\n Sample Predicted vs Actual Gaze Points:")
for i in range(5):
    print(f"Predicted: {y_pred[i]},  Actual: {y_test[i]}")

##Day 2 First exposure

#!pip install pyxdf

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

# Config
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 2/First Exposure - P1'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 100
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 100
BATCH_SIZE = 16
LEARNING_RATE = 0.001

def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f" Failed to load {filepath}: {e}")
    return None

def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y

# Collect all data once
def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)

# Load & split
print(" Loading data from XDF files...")
X, y = load_dataset(ROOT_DIR)
print(" Total sequences extracted:", len(X))

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Create datasets
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Build model
def build_model(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)  # x, y
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

print(" Building model")
model = build_model((SEQUENCE_LENGTH, FEATURE_DIM))

# Train
print(" Training model")
model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

print(" Training Complete")

history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print(" Evaluating model on test data...")

# Convert val_ds to full NumPy arrays
X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_pred = model.predict(X_test)

# Compute standard metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f"Final Test MAE (Accuracy Proxy): {mae:.4f}")

# Compute Euclidean error
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
euclidean_mean = np.mean(euclidean_distance)
print(f"Mean Euclidean Distance (error in gaze point): {euclidean_mean:.4f}")

# Plot training history
plt.figure(figsize=(10, 5))

# MSE Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train MSE')
plt.plot(history.history['val_loss'], label='Val MSE')
plt.title("MSE Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()

# MAE Plot
if 'mae' in history.history:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Train MAE')
    plt.plot(history.history['val_mae'], label='Val MAE')
    plt.title("MAE Accuracy over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Mean Absolute Error")
    plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# Convert train_ds and val_ds into NumPy arrays
X_train = np.concatenate([x for x, _ in train_ds], axis=0)
y_train = np.concatenate([y for _, y in train_ds], axis=0)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# MAE as accuracy proxy
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# Euclidean distance (for gaze)
train_euclidean = np.mean(np.linalg.norm(y_train - y_train_pred, axis=1))
test_euclidean = np.mean(np.linalg.norm(y_test - y_test_pred, axis=1))

print(" Train Metrics:")
print(f" MAE: {train_mae:.4f}")
print(f" Mean Euclidean Distance: {train_euclidean:.4f}")

print(" Test Metrics:")
print(f" MAE: {test_mae:.4f}")
print(f" Mean Euclidean Distance: {test_euclidean:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print("Evaluating model on test data.")
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f" Final Test MAE: {mae:.4f}")

# Euclidean distance
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
mean_euclidean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance: {mean_euclidean:.4f}")

#  few sample predictions
print("\n Sample Predicted vs Actual Gaze Points:")
for i in range(5):
    print(f"Predicted: {y_pred[i]},  Actual: {y_test[i]}")

##Day 2 Second exposure

!pip install pyxdf

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

# Config
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 2/Second Exposure - P2'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 100
BATCH_SIZE = 16
LEARNING_RATE = 0.001

def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f" Failed to load {filepath}: {e}")
    return None

def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y

# Collecting all data
def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)

# Load & split
print(" Loading data from XDF files.")
X, y = load_dataset(ROOT_DIR)
print("Total sequences extracted:", len(X))

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Create datasets
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Build model
def build_model(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)  # x, y
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

print(" Building model...")
model = build_model((SEQUENCE_LENGTH, FEATURE_DIM))

# Train
print("Training model...")
model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

print(" Training Complete")

history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print(" Evaluating model on test data")

# Convert val_ds to  NumPy arrays
X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_pred = model.predict(X_test)

# Computing std metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f" Final Test MAE (Accuracy Proxy): {mae:.4f}")

# Computing Euclidean error
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
euclidean_mean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance (error in gaze point): {euclidean_mean:.4f}")


plt.figure(figsize=(10, 5))

# MSE Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train MSE')
plt.plot(history.history['val_loss'], label='Val MSE')
plt.title("MSE Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()

# MAE
if 'mae' in history.history:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Train MAE')
    plt.plot(history.history['val_mae'], label='Val MAE')
    plt.title("MAE Accuracy over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Mean Absolute Error")
    plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# Convert train_ds and val_ds into NumPy arrays
X_train = np.concatenate([x for x, _ in train_ds], axis=0)
y_train = np.concatenate([y for _, y in train_ds], axis=0)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

# Predict
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# MAE
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# Euclidean distance
train_euclidean = np.mean(np.linalg.norm(y_train - y_train_pred, axis=1))
test_euclidean = np.mean(np.linalg.norm(y_test - y_test_pred, axis=1))

print(" Train Metrics:")
print(f"    MAE: {train_mae:.4f}")
print(f"    Mean Euclidean Distance: {train_euclidean:.4f}")

print("Test Metrics:")
print(f"    MAE: {test_mae:.4f}")
print(f"    Mean Euclidean Distance: {test_euclidean:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Evaluate model
print(" Evaluating model on test data...")
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f" Final Test MSE: {mse:.4f}")
print(f" Final Test MAE: {mae:.4f}")

# Euclidean distance
euclidean_distance = np.linalg.norm(y_test - y_pred, axis=1)
mean_euclidean = np.mean(euclidean_distance)
print(f" Mean Euclidean Distance: {mean_euclidean:.4f}")

#  few sample predictions
print("\n  Sample Predicted vs Actual Gaze Points:")
for i in range(5):
    print(f"Predicted: {y_pred[i]},  Actual: {y_test[i]}")

print("hi")

!pip install pyxdf

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ================================
# CONFIGURATION
# ================================
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 1/P1 - First Exposure'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 50
BATCH_SIZE = 16
LEARNING_RATE = 0.001
MODEL_TYPE = "transformer"   # choose from: cnn, lstm, cnn_lstm, bilstm, transformer
# ================================

def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f"Failed to load {filepath}: {e}")
    return None

def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y

def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)

print("üìÇ Loading data from XDF files...")
X, y = load_dataset(ROOT_DIR)
print("‚úÖ Total sequences extracted:", len(X))

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# ================================
# MODEL ARCHITECTURES
# ================================

def build_cnn(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    return model

def build_lstm(input_shape):
    model = models.Sequential([
        layers.LSTM(128, return_sequences=False, input_shape=input_shape),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    return model

def build_cnn_lstm(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.LSTM(128),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    return model

def build_bilstm(input_shape):
    model = models.Sequential([
        layers.Bidirectional(layers.LSTM(128, return_sequences=False), input_shape=input_shape),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    return model

def build_transformer(input_shape, num_heads=4, ff_dim=128):
    inputs = layers.Input(shape=input_shape)
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[-1])(x, x)
    x = layers.Add()([x, attn_output])
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    ff = layers.Dense(ff_dim, activation='relu')(x)
    ff = layers.Dense(input_shape[-1])(ff)
    x = layers.Add()([x, ff])
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(LABEL_DIM)(x)
    model = models.Model(inputs, outputs)
    return model

# ================================
# BUILD & TRAIN
# ================================
if MODEL_TYPE == "cnn":
    model = build_cnn((SEQUENCE_LENGTH, FEATURE_DIM))
elif MODEL_TYPE == "lstm":
    model = build_lstm((SEQUENCE_LENGTH, FEATURE_DIM))
elif MODEL_TYPE == "cnn_lstm":
    model = build_cnn_lstm((SEQUENCE_LENGTH, FEATURE_DIM))
elif MODEL_TYPE == "bilstm":
    model = build_bilstm((SEQUENCE_LENGTH, FEATURE_DIM))
elif MODEL_TYPE == "transformer":
    model = build_transformer((SEQUENCE_LENGTH, FEATURE_DIM))
else:
    raise ValueError("Invalid MODEL_TYPE")

model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse', metrics=['mae'])

print(f"\nüß† Training {MODEL_TYPE.upper()} model...")
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)
print("‚úÖ Training complete!")

# ================================
# EVALUATION
# ================================
X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
euclidean_mean = np.mean(np.linalg.norm(y_test - y_pred, axis=1))

print(f"\nüìä Final Evaluation for {MODEL_TYPE.upper()} model:")
print(f"  MSE: {mse:.4f}")
print(f"  MAE: {mae:.4f}")
print(f"  Mean Euclidean Distance: {euclidean_mean:.4f}")

# ================================
# VISUALIZATION
# ================================
plt.figure(figsize=(10, 4))
plt.plot(history.history['loss'], label='Train Loss (MSE)')
plt.plot(history.history['val_loss'], label='Val Loss (MSE)')
plt.title(f"{MODEL_TYPE.upper()} - Training and Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.legend()
plt.show()

#day 1 p1

# ============================================================
# üìò Unified Eye-Tracking Model Benchmark Script
# Author: Divyansh Saini
# ============================================================

!pip install pyxdf
from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ------------------- CONFIG -------------------
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 1/P1 - First Exposure'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 50
BATCH_SIZE = 16
LEARNING_RATE = 0.001
# ----------------------------------------------

# =============== DATA LOADING =================
def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f"‚ùå Failed to load {filepath}: {e}")
    return None


def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y


def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)


print("üì• Loading data from XDF files...")
X, y = load_dataset(ROOT_DIR)
print(f"‚úÖ Total sequences extracted: {len(X)}")

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

input_shape = (SEQUENCE_LENGTH, FEATURE_DIM)

# ============================================================
# üß† MODEL DEFINITIONS
# ============================================================
def build_cnn(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_lstm(input_shape):
    model = models.Sequential([
        layers.LSTM(128, return_sequences=True, input_shape=input_shape),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_cnn_lstm(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.MaxPooling1D(2),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_bilstm(input_shape):
    model = models.Sequential([
        layers.Bidirectional(layers.LSTM(128, return_sequences=True), input_shape=input_shape),
        layers.Bidirectional(layers.LSTM(64)),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_gru(input_shape):
    model = models.Sequential([
        layers.GRU(128, return_sequences=True, input_shape=input_shape),
        layers.GRU(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_transformer(input_shape, num_heads=4, ff_dim=128, num_layers=2):
    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)
        ffn = models.Sequential([
            layers.Dense(ff_dim, activation='relu'),
            layers.Dense(input_shape[-1]),
        ])
        ffn_output = ffn(x)
        x = layers.Add()([x, ffn_output])
        x = layers.LayerNormalization()(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(LABEL_DIM)(x)
    model = models.Model(inputs, outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

# ============================================================
# üöÄ TRAINING & EVALUATION LOOP
# ============================================================
models_dict = {
    "CNN": build_cnn,
    "LSTM": build_lstm,
    "CNN-LSTM": build_cnn_lstm,
    "BiLSTM": build_bilstm,
    "GRU": build_gru,
    "Transformer": build_transformer
}

results = {}

for name, builder in models_dict.items():
    print(f"\n==============================")
    print(f"üîπ Training {name} Model")
    print(f"==============================")

    model = builder(input_shape)
    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    euclidean = np.mean(np.linalg.norm(y_test - y_pred, axis=1))

    results[name] = {"MSE": mse, "MAE": mae, "Euclidean": euclidean}

    print(f"\nüìä {name} Results:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  Mean Euclidean Distance: {euclidean:.4f}")

# ============================================================
# üìà COMPARISON RESULTS
# ============================================================
print("\n\n==============================")
print("üî∏ Model Performance Summary üî∏")
print("==============================")
for k, v in results.items():
    print(f"{k:12s} | MSE: {v['MSE']:.4f} | MAE: {v['MAE']:.4f} | Euclidean: {v['Euclidean']:.4f}")

from google.colab import drive
    drive.mount('/content/drive')

#day 1 p2

# ============================================================
# üìò Unified Eye-Tracking Model Benchmark Script
# Author: Divyansh Saini
# ============================================================

!pip install pyxdf
from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ------------------- CONFIG -------------------
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 1/P2 - Second Exposure'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 50
BATCH_SIZE = 16
LEARNING_RATE = 0.001
# ----------------------------------------------

# =============== DATA LOADING =================
def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f"‚ùå Failed to load {filepath}: {e}")
    return None


def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y


def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)


print("üì• Loading data from XDF files...")
X, y = load_dataset(ROOT_DIR)
print(f"‚úÖ Total sequences extracted: {len(X)}")

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

input_shape = (SEQUENCE_LENGTH, FEATURE_DIM)

# ============================================================
# üß† MODEL DEFINITIONS
# ============================================================
def build_cnn(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_lstm(input_shape):
    model = models.Sequential([
        layers.LSTM(128, return_sequences=True, input_shape=input_shape),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_cnn_lstm(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.MaxPooling1D(2),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_bilstm(input_shape):
    model = models.Sequential([
        layers.Bidirectional(layers.LSTM(128, return_sequences=True), input_shape=input_shape),
        layers.Bidirectional(layers.LSTM(64)),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_gru(input_shape):
    model = models.Sequential([
        layers.GRU(128, return_sequences=True, input_shape=input_shape),
        layers.GRU(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_transformer(input_shape, num_heads=4, ff_dim=128, num_layers=2):
    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)
        ffn = models.Sequential([
            layers.Dense(ff_dim, activation='relu'),
            layers.Dense(input_shape[-1]),
        ])
        ffn_output = ffn(x)
        x = layers.Add()([x, ffn_output])
        x = layers.LayerNormalization()(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(LABEL_DIM)(x)
    model = models.Model(inputs, outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

# ============================================================
# üöÄ TRAINING & EVALUATION LOOP
# ============================================================
models_dict = {
    "CNN": build_cnn,
    "LSTM": build_lstm,
    "CNN-LSTM": build_cnn_lstm,
    "BiLSTM": build_bilstm,
    "GRU": build_gru,
    "Transformer": build_transformer
}

results = {}

for name, builder in models_dict.items():
    print(f"\n==============================")
    print(f"üîπ Training {name} Model")
    print(f"==============================")

    model = builder(input_shape)
    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    euclidean = np.mean(np.linalg.norm(y_test - y_pred, axis=1))

    results[name] = {"MSE": mse, "MAE": mae, "Euclidean": euclidean}

    print(f"\nüìä {name} Results:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  Mean Euclidean Distance: {euclidean:.4f}")

# ============================================================
# üìà COMPARISON RESULTS
# ============================================================
print("\n\n==============================")
print("üî∏ Model Performance Summary üî∏")
print("==============================")
for k, v in results.items():
    print(f"{k:12s} | MSE: {v['MSE']:.4f} | MAE: {v['MAE']:.4f} | Euclidean: {v['Euclidean']:.4f}")

#day 2 p1

# ============================================================
# üìò Unified Eye-Tracking Model Benchmark Script
# Author: Divyansh Saini
# ============================================================

!pip install pyxdf
from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ------------------- CONFIG -------------------
ROOT_DIR = '/content/drive/MyDrive/Lucknow Data/Day 2/First Exposure - P1'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 50
BATCH_SIZE = 16
LEARNING_RATE = 0.001
# ----------------------------------------------

# =============== DATA LOADING =================
def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f"‚ùå Failed to load {filepath}: {e}")
    return None


def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y


def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)


print("üì• Loading data from XDF files...")
X, y = load_dataset(ROOT_DIR)
print(f"‚úÖ Total sequences extracted: {len(X)}")

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

input_shape = (SEQUENCE_LENGTH, FEATURE_DIM)

# ============================================================
# üß† MODEL DEFINITIONS
# ============================================================
def build_cnn(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_lstm(input_shape):
    model = models.Sequential([
        layers.LSTM(128, return_sequences=True, input_shape=input_shape),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_cnn_lstm(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.MaxPooling1D(2),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_bilstm(input_shape):
    model = models.Sequential([
        layers.Bidirectional(layers.LSTM(128, return_sequences=True), input_shape=input_shape),
        layers.Bidirectional(layers.LSTM(64)),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_gru(input_shape):
    model = models.Sequential([
        layers.GRU(128, return_sequences=True, input_shape=input_shape),
        layers.GRU(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_transformer(input_shape, num_heads=4, ff_dim=128, num_layers=2):
    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)
        ffn = models.Sequential([
            layers.Dense(ff_dim, activation='relu'),
            layers.Dense(input_shape[-1]),
        ])
        ffn_output = ffn(x)
        x = layers.Add()([x, ffn_output])
        x = layers.LayerNormalization()(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(LABEL_DIM)(x)
    model = models.Model(inputs, outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

# ============================================================
# üöÄ TRAINING & EVALUATION LOOP
# ============================================================
models_dict = {
    "CNN": build_cnn,
    "LSTM": build_lstm,
    "CNN-LSTM": build_cnn_lstm,
    "BiLSTM": build_bilstm,
    "GRU": build_gru,
    "Transformer": build_transformer
}

results = {}

for name, builder in models_dict.items():
    print(f"\n==============================")
    print(f"üîπ Training {name} Model")
    print(f"==============================")

    model = builder(input_shape)
    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    euclidean = np.mean(np.linalg.norm(y_test - y_pred, axis=1))

    results[name] = {"MSE": mse, "MAE": mae, "Euclidean": euclidean}

    print(f"\nüìä {name} Results:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  Mean Euclidean Distance: {euclidean:.4f}")

# ============================================================
# üìà COMPARISON RESULTS
# ============================================================
print("\n\n==============================")
print("üî∏ Model Performance Summary üî∏")
print("==============================")
for k, v in results.items():
    print(f"{k:12s} | MSE: {v['MSE']:.4f} | MAE: {v['MAE']:.4f} | Euclidean: {v['Euclidean']:.4f}")

#day 2 p2

!pip install pyxdf
import os
import numpy as np
import pyxdf
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ------------------- CONFIG -------------------
ROOT_DIR = '/Users/divyanshsaini/Documents/eyeteackipynb/Lucknow Data/Lucknow Data/Day 2/Second Exposure - P2 copy'
SEQUENCE_LENGTH = 50
FEATURE_DIM = 10
LABEL_DIM = 2
MAX_SAMPLES_PER_FILE = 1000
EPOCHS = 10
BATCH_SIZE = 16
LEARNING_RATE = 0.001
# ----------------------------------------------

# =============== DATA LOADING =================
def load_webcam_data(filepath):
    try:
        streams, _ = pyxdf.load_xdf(filepath)
        for stream in streams:
            if stream['info']['name'][0] == "WebcamStream":
                data = np.array(stream['time_series'])
                if data.shape[0] > MAX_SAMPLES_PER_FILE:
                    data = data[:MAX_SAMPLES_PER_FILE]
                return data
    except Exception as e:
        print(f"‚ùå Failed to load {filepath}: {e}")
    return None


def extract_sequences_from_file(filepath):
    data = load_webcam_data(filepath)
    if data is None or data.shape[1] < (FEATURE_DIM + LABEL_DIM):
        return [], []
    X, y = [], []
    for i in range(len(data) - SEQUENCE_LENGTH):
        seq = data[i:i+SEQUENCE_LENGTH, :FEATURE_DIM]
        label = data[i+SEQUENCE_LENGTH, -LABEL_DIM:]
        X.append(seq.astype(np.float32))
        y.append(label.astype(np.float32))
    return X, y


def load_dataset(root_dir):
    all_X, all_y = [], []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.xdf'):
                file_path = os.path.join(root, file)
                X, y = extract_sequences_from_file(file_path)
                if X and y:
                    all_X.extend(X)
                    all_y.extend(y)
    return np.array(all_X), np.array(all_y)


print("üì• Loading data from XDF files...")
X, y = load_dataset(ROOT_DIR)
print(f"‚úÖ Total sequences extracted: {len(X)}")

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

X_test = np.concatenate([x for x, _ in val_ds], axis=0)
y_test = np.concatenate([y for _, y in val_ds], axis=0)

input_shape = (SEQUENCE_LENGTH, FEATURE_DIM)

# ============================================================
# üß† MODEL DEFINITIONS
# ============================================================
def build_cnn(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.GlobalAveragePooling1D(),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_lstm(input_shape):
    model = models.Sequential([
        layers.LSTM(128, return_sequences=True, input_shape=input_shape),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_cnn_lstm(input_shape):
    model = models.Sequential([
        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),
        layers.MaxPooling1D(2),
        layers.Conv1D(128, 3, activation='relu'),
        layers.MaxPooling1D(2),
        layers.LSTM(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_bilstm(input_shape):
    model = models.Sequential([
        layers.Bidirectional(layers.LSTM(128, return_sequences=True), input_shape=input_shape),
        layers.Bidirectional(layers.LSTM(64)),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_gru(input_shape):
    model = models.Sequential([
        layers.GRU(128, return_sequences=True, input_shape=input_shape),
        layers.GRU(64),
        layers.Dense(64, activation='relu'),
        layers.Dense(LABEL_DIM)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model


def build_transformer(input_shape, num_heads=4, ff_dim=128, num_layers=2):
    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)
        ffn = models.Sequential([
            layers.Dense(ff_dim, activation='relu'),
            layers.Dense(input_shape[-1]),
        ])
        ffn_output = ffn(x)
        x = layers.Add()([x, ffn_output])
        x = layers.LayerNormalization()(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(LABEL_DIM)(x)
    model = models.Model(inputs, outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='mse')
    return model

# ============================================================
# üöÄ TRAINING & EVALUATION LOOP
# ============================================================
models_dict = {
    "CNN": build_cnn,
    "LSTM": build_lstm,
    "CNN-LSTM": build_cnn_lstm,
    "BiLSTM": build_bilstm,
    "GRU": build_gru,
    "Transformer": build_transformer
}

results = {}

for name, builder in models_dict.items():
    print(f"\n==============================")
    print(f"üîπ Training {name} Model")
    print(f"==============================")

    model = builder(input_shape)
    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    euclidean = np.mean(np.linalg.norm(y_test - y_pred, axis=1))

    results[name] = {"MSE": mse, "MAE": mae, "Euclidean": euclidean}

    print(f"\nüìä {name} Results:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  Mean Euclidean Distance: {euclidean:.4f}")

# ============================================================
# üìà COMPARISON RESULTS
# ============================================================
print("\n\n==============================")
print("üî∏ Model Performance Summary üî∏")
print("==============================")
for k, v in results.items():
    print(f"{k:12s} | MSE: {v['MSE']:.4f} | MAE: {v['MAE']:.4f} | Euclidean: {v['Euclidean']:.4f}")

""" CNN Results:
  MSE: 0.0007
  MAE: 0.0207
  Mean Euclidean Distance: 0.0363




   LSTM Results:
  MSE: 0.0000
  MAE: 0.0037
  Mean Euclidean Distance: 0.0053
...



CNN-LSTM Results:
  MSE: 0.0000
  MAE: 0.0040
  Mean Euclidean Distance: 0.0059




  BiLSTM Results:
  MSE: 0.0000
  MAE: 0.0037
  Mean Euclidean Distance: 0.0053
...



...
CNN-LSTM     | MSE: 0.0000 | MAE: 0.0040 | Euclidean: 0.0059
BiLSTM       | MSE: 0.0000 | MAE: 0.0037 | Euclidean: 0.0053
GRU          | MSE: 0.0000 | MAE: 0.0052 | Euclidean: 0.0083
Transformer  | MSE: 0.0000 | MAE: 0.0043 | Euclidean: 0.0060
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
...
"""

