# -*- coding: utf-8 -*-
"""Web_gazer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wIYaBd3-r35o87Z8ubpH8Lt6BN_o9nQf
"""

#web gazer Dataset

from google.colab import drive
drive.mount('/content/drive')



#Training the model for 10 epochs only using adam classifier for random labels  for demo

import os
import cv2
import numpy as np
import random
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm

# Parameters
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
img_size = 64
max_frames_per_video = 100

# load video frames
X = []
y = []

print(" Extracting frames and generating labels \n ")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_count = 0

            while cap.isOpened() and frame_count < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.resize(frame, (img_size, img_size))
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                X.append(frame)

                # Dummy label (0 or 1)
                y.append(random.randint(0, 1))
                frame_count += 1

            cap.release()

X = np.array(X).reshape(-1, img_size, img_size, 1) / 255.0  # normalize
y = np.array(y)
y = to_categorical(y, num_classes=2)

print(f" Total samples \n: {X.shape[0]}")


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


print(" Training started \n ")
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)


loss, accuracy = model.evaluate(X_test, y_test)
print(f"\n Test Loss: {loss:.4f}, Accuracy: {accuracy*100:.2f}%")


import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('Accuracy')
plt.legend()
plt.show()



##Training the model for 100 epochs  using adam classifier

import os
import cv2
import numpy as np
import random
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm

#  Parameters
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
img_size = 64
max_frames_per_video = 100

# load video frames
X = []
y = []

print(" Extracting frames and generating labels.")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_count = 0

            while cap.isOpened() and frame_count < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.resize(frame, (img_size, img_size))
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                X.append(frame)


                y.append(random.randint(0, 1))
                frame_count += 1

            cap.release()

X = np.array(X).reshape(-1, img_size, img_size, 1) / 255.0  # normalize
y = np.array(y)
y = to_categorical(y, num_classes=2)

print(f" Total samples: {X.shape[0]}")

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CNN Model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#  Training  the model
print(" Training started \n ")
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

#  Evaluation
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\n Test Loss: {loss:.4f}, Accuracy: {accuracy*100:.2f}%")

# Plot metrics
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('Accuracy')
plt.legend()
plt.show()



#read csv file

import os
import pandas as pd

# main folder
main_folder = '/content/drive/My Drive/WebGazerETRA2018Dataset_Release20180420'


for file in os.listdir(main_folder):
    if file.endswith('.csv'):
        csv_path = os.path.join(main_folder, file)
        break

df = pd.read_csv(csv_path)


print("\n  CSV File Loaded:")
print(df.head())

#rain a CNN model to classify each video frame as belonging to a participant who:

#Wears glasses → label 1

#Doesn't wear glasses label 0

import os
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
import random
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Parameters
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
csv_path = os.path.join(data_dir, '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420/participant_characteristics.csv')
img_size = 64
max_frames_per_video = 100

# read csv
df = pd.read_csv(csv_path)
df.set_index('Participant ID', inplace=True)

# Label mapping
def get_label(participant_id):
    try:
        vision_status = df.loc[participant_id]['Self-Reported Vision']
        if 'glasses' in str(vision_status).lower():
            return 1
        return 0
    except:
        return None  # Skip if no info

#  Extract Frames using csv
X = []
y = []

print(" \n  Extracting frames and applying CSV-based labels.")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    label = get_label(participant)
    if label is None:
        continue  # skip participants with no valid label

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_count = 0

            while cap.isOpened() and frame_count < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.resize(frame, (img_size, img_size))
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                X.append(frame)
                y.append(label)
                frame_count += 1
            cap.release()

#  Preprocessing
X = np.array(X).reshape(-1, img_size, img_size, 1) / 255.0
y = to_categorical(y, num_classes=2)

print(f"\n  Loaded: {X.shape[0]} samples")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  CNN Model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(2, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#  Train the model
print(" \n Training started.")
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

#  Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\n Test Loss: {loss:.4f}, Accuracy: {accuracy*100:.2f}%")

# Plotting
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()





df = pd.read_csv(csv_path)
print(" CSV Columns:", df.columns.tolist())

# installing media pipe

pip install mediapipe opencv-python

# gaze labels for 2d est.



import cv2
import mediapipe as mp
import numpy as np
import os
import pandas as pd
from tqdm import tqdm

# Paths
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
output_csv = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420/gaze_labels.csv'
img_size = 64
max_frames_per_video = 100

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)

gaze_labels = []

print("\n  Generating gaze labels.")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_index = 0

            while cap.isOpened() and frame_index < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break

                try:
                    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    results = face_mesh.process(img_rgb)

                    if results.multi_face_landmarks:
                        landmarks = results.multi_face_landmarks[0].landmark

                        if len(landmarks) > 473:
                            left_eye = landmarks[468]
                            right_eye = landmarks[473]

                            # Avg gaze coordinates (x, y between 0 and 1)
                            gaze_x = (left_eye.x + right_eye.x) / 2
                            gaze_y = (left_eye.y + right_eye.y) / 2

                            gaze_labels.append({
                                'Participant ID': participant,
                                'Video Name': file,
                                'Frame Index': frame_index,
                                'Gaze X': gaze_x,
                                'Gaze Y': gaze_y
                            })
                except Exception as e:
                    print(f"\n  Skipping frame {frame_index} in {file} due to error: {e}")

                frame_index += 1

            cap.release()

# Save to CSV
df_labels = pd.DataFrame(gaze_labels)
df_labels.to_csv(output_csv, index=False)
print(f"\n Gaze labels saved to: {output_csv}")

##read label csv file head

print(df_labels.head())

##cnn model for 2d estimation

import os
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import matplotlib.pyplot as plt

# Parameters
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
csv_path = os.path.join(data_dir, 'gaze_labels.csv')
img_size = 64
max_frames_per_video = 100

# load gaze labels
df = pd.read_csv(csv_path)

#  Extract frames and match gaze labels
X = []
y = []

print("\n Extracting frames and applying gaze labels.")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_index = 0

            while cap.isOpened() and frame_index < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break

                try:
                    # Match with gaze label
                    row = df[
                        (df['Participant ID'] == participant) &
                        (df['Video Name'] == file) &
                        (df['Frame Index'] == frame_index)
                    ]

                    if row.empty:
                        frame_index += 1
                        continue

                    gaze_x = row['Gaze X'].values[0]
                    gaze_y = row['Gaze Y'].values[0]

                    # Processing  frame
                    frame = cv2.resize(frame, (img_size, img_size))
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    X.append(frame)
                    y.append([gaze_x, gaze_y])

                except Exception as e:
                    print(f" Error on {file} frame {frame_index}: {e}")

                frame_index += 1
            cap.release()

# Preprocessing
X = np.array(X).reshape(-1, img_size, img_size, 1) / 255.0
y = np.array(y)  # shape: [samples, 2]

print(f"\n  Loaded {X.shape[0]} frames")

#  Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  CNN Model for 2D regression
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(2)  # Predict [Gaze X, Gaze Y]
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
print("\n  Training started.")
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

# Evaluate
loss, mae = model.evaluate(X_test, y_test)
print(f"\n  Test MSE: {loss:.4f}, MAE: {mae:.4f}")

#  Plot loss and MAE
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Mean Squared Error')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.title('Mean Absolute Error')
plt.legend()

plt.tight_layout()
plt.show()

# 3d gaze labels

import cv2
import mediapipe as mp
import numpy as np
import os
import pandas as pd
from tqdm import tqdm

# Paths
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
output_csv = os.path.join(data_dir, '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420/gaze_labels3d.csv')
img_size = 64
max_frames_per_video = 100

# Mediapipe
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)

gaze_labels_3d = []

print("\n Generating 3D gaze labels.")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_index = 0

            while cap.isOpened() and frame_index < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break

                try:
                    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    results = face_mesh.process(img_rgb)

                    if results.multi_face_landmarks:
                        landmarks = results.multi_face_landmarks[0].landmark
                        if len(landmarks) > 473:
                            left_eye = landmarks[468]
                            right_eye = landmarks[473]

                            # Get average eye center
                            eye_x = (left_eye.x + right_eye.x) / 2
                            eye_y = (left_eye.y + right_eye.y) / 2


                            eye_z = 1.0

                            # Construct and normalize 3D vector
                            vec = np.array([eye_x - 0.5, eye_y - 0.5, eye_z])  # centered at (0.5, 0.5)
                            norm_vec = vec / np.linalg.norm(vec)

                            gaze_labels_3d.append({
                                'Participant ID': participant,
                                'Video Name': file,
                                'Frame Index': frame_index,
                                'Gaze X': norm_vec[0],
                                'Gaze Y': norm_vec[1],
                                'Gaze Z': norm_vec[2]
                            })

                except Exception as e:
                    print(f"\n  Frame {frame_index} in {file} skipped due to: {e}")

                frame_index += 1

            cap.release()

# Save to CSV
df_labels = pd.DataFrame(gaze_labels_3d)
df_labels.to_csv(output_csv, index=False)
print(f"\n 3D Gaze labels saved to: {output_csv}")

#training cnn model for prediction of 3d gaze

import os
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import matplotlib.pyplot as plt
from numpy.linalg import norm

# Parameters
data_dir = '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420'
csv_path = os.path.join(data_dir, '/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420/gaze_labels3d.csv')
img_size = 64
max_frames_per_video = 100

# Load gaze labels
df = pd.read_csv(csv_path)

# Prepare data
X = []
y = []

print("\n Extracting frames and applying 3D gaze labels...")
for participant in tqdm(sorted(os.listdir(data_dir))):
    part_path = os.path.join(data_dir, participant)
    if not os.path.isdir(part_path):
        continue

    for file in os.listdir(part_path):
        if file.endswith(".webm"):
            video_path = os.path.join(part_path, file)
            cap = cv2.VideoCapture(video_path)
            frame_index = 0

            while cap.isOpened() and frame_index < max_frames_per_video:
                ret, frame = cap.read()
                if not ret:
                    break

                row = df[
                    (df['Participant ID'] == participant) &
                    (df['Video Name'] == file) &
                    (df['Frame Index'] == frame_index)
                ]

                if row.empty:
                    frame_index += 1
                    continue

                try:
                    gaze_x = row['Gaze X'].values[0]
                    gaze_y = row['Gaze Y'].values[0]
                    gaze_z = row['Gaze Z'].values[0]

                    frame = cv2.resize(frame, (img_size, img_size))
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

                    X.append(frame)
                    y.append([gaze_x, gaze_y, gaze_z])
                except Exception as e:
                    print(f" Error at frame {frame_index} in {file}: {e}")

                frame_index += 1
            cap.release()

# Preprocess
X = np.array(X).reshape(-1, img_size, img_size, 1) / 255.0
y = np.array(y)

print(f"\n Loaded {X.shape[0]} samples")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CNN Model for 3D reg
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(3)  #  Gaze X, Y, Z
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
print("\n Training 3D model ")
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

# Evaluate
loss, mae = model.evaluate(X_test, y_test)
print(f"\n  Test MSE: {loss:.4f}, MAE: {mae:.4f}")

# Angular error in 3D
print("\n  Calculating Mean Angular Error (3D)")
y_pred = model.predict(X_test)

dot_products = np.sum(y_pred * y_test, axis=1)
norm_pred = norm(y_pred, axis=1)
norm_true = norm(y_test, axis=1)

cos_sim = dot_products / (norm_pred * norm_true + 1e-8)
angles = np.arccos(np.clip(cos_sim, -1.0, 1.0))  # Radians
angular_error_deg = np.degrees(angles)

mean_ang_error = np.mean(angular_error_deg)
print(f" Mean Angular Error: {mean_ang_error:.2f}°")

# Plot loss and MAE
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Mean Squared Error')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.title('Mean Absolute Error')
plt.legend()

plt.tight_layout()
plt.show()

df=pd.read_csv('/content/drive/MyDrive/WebGazerETRA2018Dataset_Release20180420/gaze_labels3d.csv')
print(df_labels.head)

